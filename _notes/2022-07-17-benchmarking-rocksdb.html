---
layout: post
title: "Benchmarking RocksDB"
short: "Notes related to Benchmarking RocksDB"
category: benchmarking
status: Stale
---

<p>
This page contains various notes related to benchmarking RocksDB.
RocksDB can be benchmarked with the help of db<sub>bench</sub>. This is a tool made originally made by Google for LevelDB, but now altered for Meta&rsquo;s workload with RocksDB. As it closely matches the needs of Meta, it is a handy tool to use when you want to test a key-value store that is not production-ready yet or will not start with a large workload out of the box. For example if you intend to use RocksDB, there are quite a few different options to pick from, to see what works best db<sub>bench</sub> can help significantly.
</p>

<div id="outline-container-orgfb4dadb" class="outline-2">
<h2 id="orgfb4dadb"><span class="section-number-2">1.</span> !!!This page requires cleaning!!!</h2>
</div>

<div id="outline-container-org3fd0ef1" class="outline-2">
<h2 id="org3fd0ef1"><span class="section-number-2">2.</span> Setup db<sub>bench</sub></h2>
<div class="outline-text-2" id="text-2">
<p>
db<sub>bench</sub> should come along with the rest of the RocksDB repository. By using CMake it should be possible to only issue one build command and all of db<sub>bench</sub> dependencies will be built as well. Do ensure that is run in release mode. An example command would be:
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #ECBE7B;">mkdir</span> build
<span style="color: #ECBE7B;">cd</span> build
cmake -DCMAKE_BUILD_TYPE=Release ..
<span style="color: #ECBE7B;">make</span> -j db_bench
</pre>
</div>
</div>

<div id="outline-container-orgbedaae1" class="outline-3">
<h3 id="orgbedaae1"><span class="section-number-3">2.1.</span> Improve histogram to show more information</h3>
<div class="outline-text-3" id="text-2-1">
<p>
db<sub>bench</sub> has an option known as &#x2013;histogram. This option is great for general analysis, but misses a lot of information. For example, when creating a box plot you also want the P25, which is not available by default. It is also hard to measure tail latency, as P99.99 is not enough. This can (at the moment) be altered as follows:
</p>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #ECBE7B;">cd</span> rocksdb
vi ./monitoring/histogram.cc
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">navigate with a search function to a line containing "99."</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">add the percentiles you need. E.g. "P25 %.2f" at the beginning and add Percentile(25) as first argument</span>
<span style="color: #ECBE7B;">make</span> -j db_bench
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgb18a910" class="outline-2">
<h2 id="orgb18a910"><span class="section-number-2">3.</span> Interesting workloads</h2>
<div class="outline-text-2" id="text-3">
<p>
db<sub>bench</sub> comes with a lot of different benchmarks. It is possible to do multiple, by using &ldquo;,&rdquo; in the argument. For example benchmarks=&ldquo;fillseq,fillrandom&rdquo;. Some benchmarks that I find interesting to test the underlying storage:
</p>

<ul class="org-ul">
<li>fillseq: fills key-value pairs in sequential order. Ideal to test a key-value store in alpha stage and to test random vs non-random I/O.</li>
<li>fillrandom: fills key-value pairs in random order. Some keys MIGHT occur multiple times, requiring overwrite functionality.
overwrite: keeps overwriting a set of the same keys. This is a good stress test. Especially to test garbage collection of the LSM-tree.</li>
<li>readwhilewriting: tests resource contention. It uses x readers that all try to read, while another process keeps writing. It can be used to test for excessive locking and scaling issues.</li>
<li>stats: returns the stats of the last run.</li>
</ul>
</div>
</div>


<div id="outline-container-org0d475f4" class="outline-2">
<h2 id="org0d475f4"><span class="section-number-2">4.</span> Interesting parameters</h2>
<div class="outline-text-2" id="text-4">
<p>
db<sub>bench</sub> allows testing various configurations. A few that I use:
</p>

<ul class="org-ul">
<li>num: number of key-value pairs. This is the parameter to set when testing for large I/O (e.g. TBs of data)</li>
<li>value<sub>size</sub>: sets the value<sub>size</sub> in bytes for each key-value pair. Can be used to check how well the store performs with different types of data. Can also be interesting to check the relation between e.g. blocksize and kv-pair size.</li>
<li>key<sub>size</sub>: sets the key<sub>size</sub> in bytes for a key-value pair. This is interesting for similar reasons as the value<sub>size</sub>, but also because keys are used in the metadata of e.g. SSTables. When the key size becomes very large, so does the metadata that comes along with it. This can be used to test if the key-value store support large keys and what quantity.</li>
<li>compression<sub>type</sub>: set it to `none`` when you do not want compression.</li>
<li>write<sub>buffer</sub><sub>size</sub>: size of the buffer that is used for the memtable. The bigger it is set, the more will be kept in RAM and the less flushes are needed (albeit larger flushes are done in the end).</li>
<li>target<sub>file</sub><sub>size</sub><sub>base</sub>: the approximate size to use for files. This can be used to for example get SSTables to be approximately the size of an SSD zone.</li>
<li>histogram: very important. Shows a histogram at the end of each benchmark, that represents latencies. This is a good tool to get tail latencies of the key-value store.</li>
<li>use<sub>direct</sub><sub>reads</sub>: uses direct I/O for reads. A good parameter to set when you want to test storage.</li>
<li>use<sub>direct</sub><sub>io</sub><sub>for</sub><sub>flush</sub><sub>and</sub><sub>compaction</sub>: uses direct I/O for background operations. A good parameter to set when you want to test storage.</li>
<li>max<sub>background</sub><sub>jobs</sub>: can be used to tweak the maximum amount of background jobs that can be active. These threads can be used by e.g. compaction or flush threads, but might interfere with each other or the client threads.</li>
<li>max<sub>bytes</sub><sub>for</sub><sub>level</sub><sub>multiplier</sub>: The difference in max bytes for each level. Can be used to tweak the step between levels.</li>
<li>threads: dependent on the benchmark. Generally means the client threads. For example for write benchmarks, it refers to concurrent client writes.</li>
<li>use<sub>existing</sub><sub>db</sub>: if set will use an already existing db. Some benchmarks require an existing db, others requires a new db. This parameter is good to set if multiple benchmarks will be run in series and they should continue on a dirty state.</li>
</ul>
</div>
</div>

<div id="outline-container-org2d6e171" class="outline-2">
<h2 id="org2d6e171"><span class="section-number-2">5.</span> Benchmark RocksDB with ZenFS</h2>
<div class="outline-text-2" id="text-5">
<p>
ZenFS is an alternative file system that can only be used within RocksDB. It is essentially a domain-specific filesystem. Further on, it only works with ZNS SSDs. However, as of July 2022 this is the state of the art for RocksDB on such devices. It is therefore tempting to compare against ZenFS when benchmarking on ZNS SSDs.
</p>
</div>
<div id="outline-container-org160c093" class="outline-3">
<h3 id="org160c093"><span class="section-number-3">5.1.</span> Installing ZenFS</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Using ZenFS does require some extra installations. It must be used as a plugin for RocksDB, which also requires rebuilding RocksDB yourself. So it is not possible to use RocksDB from a package repository! At <a href="https://github.com/westerndigitalcorporation/zenfs">https://github.com/westerndigitalcorporation/zenfs</a>, it is explained how you can setup ZenFS. Nevertheless, as is always the case with rebuilding from source, things can go wrong. As far as I am aware WD (the company behind ZenFS) has no direct influence over the updates of RocksDB and the team that works on RocksDB has no direct influence over ZenFS. They are essentially separate projects. Therefore there might be versioning inconsistencies. Whenever working with ZenFS always verify that the both versions are compatible and if you benchmark, make sure that you are using a combination that is &ldquo;optimal&rdquo;. Preferably pin a commit, use this commit in your benchmarks and report the commit SHA in your experiment details.
</p>

<p>
Below I highlight some issues that I came across when using ZenFS.
</p>

<p>
The first issue is related to LibZBD. This is a dependency of ZenFS. This library should be compiled and installed manually. See <a href="https://github.com/westerndigitalcorporation/libzbd">https://github.com/westerndigitalcorporation/libzbd</a>. Be sure that ZenFS is compatible with the version of LibZBD.
</p>

<p>
If you get errors to `undefined reference to google:FlagRegisterer::Flag&#x2026;`, something probably went wrong with your gflags installation (you should already have a gflags installed as otherwise RocksDB would not compile). A good guide to solve this issue is <a href="https://github.com/gflags/gflags/issues/203">https://github.com/gflags/gflags/issues/203</a> the answer by EricOops is a life saver. EricOops recommends building and installing `gflags` and `glog` manually, which are a ZenFS dependency. However, do not forget to first purge the current gflags installations just to be sure.
</p>

<p>
Another issue has to do with the built system of ZenFS. ZenFS makes use of the plugin functionality of RocksDB. RocksDB supports plugins in both UNIX Makefiles and CMake, but there functionality differs significantly. For example the CMake plugins do not support custom scripts and do not install plugin header files on an install. Further on, if installing using the Makefile, package-config files will be created as well, while they are not when using CMake. ZenFS relies on all of these functionalities. This means that if you altered RocksDB and only use the CMakeList you have a problem. If possible only use the Makefile.
</p>

<p>
Else, consider if it is possible to benchmark RocksDB with another installation. One that use the same version, but without your changes.
</p>

<p>
If you need the changes, there is a hack to circumvent the issue, but it is not advisable. Generally it might break, but it is a good way to debug your system. Do NOT do this for benchmarking. In that case just use the Makefile.
</p>
<ol class="org-ol">
<li>Go to the zenfs directory and call `./generate-version.sh`. This would ordinarily be called from the Makefile, which we will not use.</li>
<li>Create a CMakeLists.txt in this directory. We are going to create a CMake plugin for ZenFS. Use something like (depending on the ZenFS version). Try to mimick ZenFS&rsquo; Makefile:</li>
</ol>
<div class="org-src-container">
<pre class="src src-cmake"><span style="color: #c678dd;">cmake_minimum_required</span>(VERSION <span style="color: #da8548; font-weight: bold;">3.4</span>)

<span style="color: #c678dd;">set</span>(zenfs_SOURCES <span style="color: #98be65;">"fs/fs_zenfs.cc"</span> <span style="color: #98be65;">"fs/zbd_zenfs.cc"</span> <span style="color: #98be65;">"fs/io_zenfs.cc"</span> PARENT_SCOPE)
<span style="color: #c678dd;">set</span>(zenfs_HEADER_DIR <span style="color: #98be65;">"fs"</span> PARENT_SCOPE)
<span style="color: #c678dd;">set</span>(zenfs_CMAKE_EXE_LINKER_FLAGS <span style="color: #98be65;">"-u zenfs_filesystem_reg"</span> PARENT_SCOPE)
<span style="color: #c678dd;">set</span>(zenfs_LIBS <span style="color: #98be65;">"zbd"</span> PARENT_SCOPE)
</pre>
</div>
<ol class="org-ol">
<li><p>
Go to the RocksDB directory. Alter the CMakeList to support plugin headers being copied as well: Before the `foreach(plugin \\({PLUGINS})` add `set(PLUGIN_HEADER_DIR "")` and in the loop add `list(APPEND PLUGIN_HEADER_DIR "plugin/\){plugin}/\\({
   \\){plugin}<sub>HEADER</sub><sub>DIR</sub>}&ldquo;)`. Then in the `install` section (such as at `install(TARGETS ${ROCKSDB<sub>STATIC</sub><sub>LIB</sub>}` add:
</p>
<div class="org-src-container">
<pre class="src src-cmake"><span style="color: #51afef;">foreach</span>(header_dir ${<span style="color: #dcaeea;">PLUGIN_HEADER_DIR</span>})
    <span style="color: #c678dd;">install</span>(DIRECTORY ${<span style="color: #dcaeea;">header_dir</span>} COMPONENT devel DESTINATION <span style="color: #98be65;">"${</span><span style="color: #dcaeea;">CMAKE_INSTALL_INCLUDEDIR</span><span style="color: #98be65;">}/rocksdb/plugin/zenfs"</span>)
<span style="color: #51afef;">endforeach</span>()
</pre>
</div></li>
<li>Create a copy of the project, but without your changes. Built and install it with the Makefile and ZenFS and copy the .pc file to a secure installation.</li>
<li>Deinstall that project.</li>
<li>Remove the CMakeCache of your own project and rebuild the project with the ZenFS plugin enabled. Now copy the location of the earlier generated .pc file to your PKGCONFIG path and create the ZenFS filesystem as usual. In this scenario, you will use all the configs that you use within CMake, BUT you will create the filesystem with a fallback method.</li>
</ol>
</div>
</div>

<div id="outline-container-org02d19a1" class="outline-3">
<h3 id="org02d19a1"><span class="section-number-3">5.2.</span> ZenFS Benchmarking</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Then after ZenFS is up and running. We should be able to do benchmarking. First pick an appropriate ZNS SSD and use its name. Pick the name in `/dev/&lt;name&gt;` as ZenFS automatically uses `/dev/`. Then do the following to setup ZenFS:
</p>
<div class="org-src-container">
<pre class="src src-bash"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">In all lines &lt;dev&gt; is the devicename</span>
<span style="color: #ECBE7B;">echo</span> deadline | <span style="color: #ECBE7B;">sudo</span> tee <span style="color: #98be65;">"/sys/block/</span><span style="color: #a9a1e1;">$</span><span style="color: #dcaeea;">dev</span><span style="color: #98be65;">/queue/scheduler"</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">ZenFS requires deadline as a scheduler</span>
<span style="color: #ECBE7B;">rm</span> -rf /tmp/zenfs-aux <span style="color: #5B6268;"># </span><span style="color: #5B6268;">ZenFS requires a temporary LOG file, but it is not allowed to already exist!</span>
<span style="color: #ECBE7B;">cd</span> $<span style="color: #dcaeea;">ZENFS_DIR</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">ZENFS_DIR should be the utils directory of ZenFS</span>
./zenfs mkfs -zbd=$<span style="color: #dcaeea;">dev</span> -aux_path=/tmp/zenfs-aux
</pre>
</div>
<p>
When this succeeds, you should see a message such as: &ldquo;ZenFS file system created. Free space: 3968745 MB&rdquo;. Otherwise, assume that it has failed.
</p>

<p>
Now benchmarks can be run on ZenFS. For some good examples go to <a href="https://github.com/westerndigitalcorporation/zenfs/tree/master/tests">https://github.com/westerndigitalcorporation/zenfs/tree/master/tests</a>. In particular look at `get<sub>good</sub><sub>db</sub><sub>bench</sub><sub>params</sub><sub>for</sub><sub>zenfs.sh</sub>`. What is immediately noticeable is that using ZenFS requires different db<sub>bench</sub> commands. You should modify the fs<sub>uri</sub> to point to the ZenFS filesystem with the arg `-fs<sub>uri</sub>=zenfs://dev:$dev` with `dev` the device name. Then it should already work, but it is not optimal. In addition, we should set the target filesize to equal approximately the size of a zone. This size should then be used in the arg: `&#x2013;target<sub>file</sub><sub>size</sub><sub>base</sub>`. The write buffersize, set with `&#x2013;write<sub>buffer</sub><sub>size</sub>`, should also approximate this size.
</p>
</div>
</div>
<div id="outline-container-org2d7ba0e" class="outline-3">
<h3 id="org2d7ba0e"><span class="section-number-3">5.3.</span> Remove ZenFS filesystem from a device</h3>
<div class="outline-text-3" id="text-5-3">
<p>
This is very easy as the ZenFS filesystem is never mounted. It runs in user space. So you do not have to do anything. If however, you want the ZNS SSD to go to a clean state, reset all zones with:
</p>
<div class="org-src-container">
<pre class="src src-bash">nvme zns reset-zone /dev/$<span style="color: #dcaeea;">dev</span> -a
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org6ffdd27" class="outline-2">
<h2 id="org6ffdd27"><span class="section-number-2">6.</span> Benchmarking RocksDB with F2FS and ZNS SSDs</h2>
<div class="outline-text-2" id="text-6">
<p>
F2FS supports ZNS SSDs out of the box, provided a recent version of F2FS is used. However, it does require some additional setup and things to keep track of.The first idiosyncracy is that F2FS supports sequential zones for most of its data, except for at least a part of the metadata used. ZNS can support a few zones that can be written to randomly, but does not require to support them. Further on, such zones may not be enough to hold all metadata. Whenever the amount of randomly writable space is not enough, F2FS should warn you by default. For example, 100GB requires at least 4GB of random space and 7TB requires at least 16GB of random space. When the amount of space is not enough, we have to use an additional device as there is no other way. This does hinder benchmarks as F2FS &ldquo;cheats&rdquo; in this regard. To keep side-effects to a minimum, try to use a NVMe device with similar performance.
</p>
</div>

<div id="outline-container-org006c6bc" class="outline-3">
<h3 id="org006c6bc"><span class="section-number-3">6.1.</span> Install F2FS</h3>
<div class="outline-text-3" id="text-6-1">
<p>
When using F2FS with F2fS-tools in 2022 and using the default kernel, ZNS is not supported by default. In that case, F2FS needs to be built manually. In this case, we have to be careful. Do NOT use the version on github as it does not seem to be maintained, instead clone from git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs-tools.git. Then it should be as simple as following the instructions from the repo. One thing to be aware of is if you have all dependencies properly installed and set. During the configuration phase (`/configure.sh``) you should see a list of capabilities with &ldquo;yes&rdquo; or &ldquo;no&rdquo; next to it. If &ldquo;blkzoned.capacity&rdquo; is no, you can create a ZNS file system (at least the command completes without errors or warnings), but you can not actually use it&#x2026; In this case, be sure you have a modern kernel, the headers are installed and the kernel is built with all of the required configs. In my case I had to also update a few libraries, such as BPF.
</p>
</div>
</div>
<div id="outline-container-org9bf2845" class="outline-3">
<h3 id="org9bf2845"><span class="section-number-3">6.2.</span> Create the file system</h3>
<div class="outline-text-3" id="text-6-2">
<p>
To create the filesystem on ZNS, Nick Tehrany has an excellent paper on how to use F2FS on ZNS at <a href="https://arxiv.org/abs/2206.01547">https://arxiv.org/abs/2206.01547</a>. First ensure that the ZNS device is actually empty! As at the moment (June 2022) F2FS makes no attempt to reset the device with for example:
</p>
<div class="org-src-container">
<pre class="src src-bash">nvme zns reset-zone /dev/$<span style="color: #dcaeea;">dev</span> -a replace dev with the ZNS device and do this for every namespace used by the filesystem.
</pre>
</div>
<p>
After this it is sufficient to say:
</p>
<div class="org-src-container">
<pre class="src src-bash">mkfs.f2fs -f -m -c /dev/$<span style="color: #dcaeea;">devzns</span> /dev/$<span style="color: #dcaeea;">devnvme</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">With devzns the seq-only ZNS namespace of a ZNS device and devnvme the randomly write-able namespace of a ZNS device (or an other ordinary device)</span>
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">+END_SRC bash</span>
Then mount the filesystem at the preferred mount point, such as /mnt/f2fs with:
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">+BEGIN_SRC bash</span>
mount -t f2fs /dev/$<span style="color: #dcaeea;">devnvme</span> /mnt/f2fs <span style="color: #5B6268;"># </span><span style="color: #5B6268;">with devnvme the device the randomly writeable area defined in the previous command.</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org5e919df" class="outline-3">
<h3 id="org5e919df"><span class="section-number-3">6.3.</span> Benchmarking wtih F2FS</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Be sure to use the pointed mounted with F2FS only, by specifying the db and wal directory with setting `&#x2013;db` and `&#x2013;wal<sub>dir</sub>` to be directories within the mounted filesystem.
</p>
</div>
</div>
</div>
