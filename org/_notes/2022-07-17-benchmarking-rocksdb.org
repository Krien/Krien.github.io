#+TITLE: 2022 07 17 Benchmarking Rocksdb
#+OPTIONS: toc:nil
#+BEGIN_EXPORT html
---
layout: post
title: "Benchmarking RocksDB"
short: "Notes related to Benchmarking RocksDB"
category: benchmarking
status: Stale
---
#+END_EXPORT

This page contains various notes related to benchmarking RocksDB.
RocksDB can be benchmarked with the help of db_bench. This is a tool made originally made by Google for LevelDB, but now altered for Meta's workload with RocksDB. As it closely matches the needs of Meta, it is a handy tool to use when you want to test a key-value store that is not production-ready yet or will not start with a large workload out of the box. For example if you intend to use RocksDB, there are quite a few different options to pick from, to see what works best db_bench can help significantly.

* !!!This page requires cleaning!!!

* Setup db_bench
db_bench should come along with the rest of the RocksDB repository. By using CMake it should be possible to only issue one build command and all of db_bench dependencies will be built as well. Do ensure that is run in release mode. An example command would be:

#+BEGIN_SRC bash
mkdir build
cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make -j $nprocs db_bench
#+END_SRC

If you prefer UNIX Makefiles, do:
#+BEGIN_SRC bash
DEBUG_LEVEL=0 make -j $nprocs db_bench
#+END_SRC

** Gflags
RocksDB tools such as db_bench depend on Gflags to be installed. This is leads to problems when you do not want to globally install Gflags (why would you?).
To solve this, you need to clone Gflags, install it to a custom directory, and change some variables in the build variables used for RocksDB.
#+BEGIN_SRC bash
git clone https://github.com/gflags/gflags.git
git checkout v2.0
./configure --prefix=<prefix_dir>
make
make install
#+END_SRC
If for any chance you installed the wrong version, you might need to cleanup manually.
Remove the directory `prefix_dir/include/gflags` and all `gflags` .a and .so files in `prefix_dir/lib/`.

Then built db_bench. For CMake set (e.g. ccmake) `GFLAGS_INCLUDE_DIR` to `prefix_dir/include/gflags` and `GFLAGS_LIBRARIES` to `prefix_dir/lib/libgflags.a` (so use the static one).
For the Unix Makefile, the following seems to work:
#+BEGIN_SRC bash
make clean # Always clean before rebuilding!
PKG_CONFIG_PATH="$PKG_CONFIG_PATH:<prefix_dir>/lib/pkgconfig" CFLAGS="-I <prefix_dir>/include/gflags/ -DGFLAGS=google" LDFLAGS="<prefix_dir>/lib/libgflags.a" <OTHER_VARS> make db_bench
#+END_SRC


** Improve histogram to show more information

db_bench has an option known as --histogram. This option is great for general analysis, but misses a lot of information. For example, when creating a box plot you also want the P25, which is not available by default. It is also hard to measure tail latency, as P99.99 is not enough. This can (at the moment) be altered as follows:

#+BEGIN_SRC bash
cd rocksdb
vi ./monitoring/histogram.cc
# navigate with a search function to a line containing "99."
# add the percentiles you need. E.g. "P25 %.2f" at the beginning and add Percentile(25) as first argument
make -j db_bench
#+END_SRC

* Interesting workloads
db_bench comes with a lot of different benchmarks. It is possible to do multiple, by using "," in the argument. For example benchmarks="fillseq,fillrandom". Some benchmarks that I find interesting to test the underlying storage:

- fillseq: fills key-value pairs in sequential order. Ideal to test a key-value store in alpha stage and to test random vs non-random I/O.
- fillrandom: fills key-value pairs in random order. Some keys MIGHT occur multiple times, requiring overwrite functionality.
    overwrite: keeps overwriting a set of the same keys. This is a good stress test. Especially to test garbage collection of the LSM-tree.
- readwhilewriting: tests resource contention. It uses x readers that all try to read, while another process keeps writing. It can be used to test for excessive locking and scaling issues.
- stats: returns the stats of the last run.


* Interesting parameters
db_bench allows testing various configurations. A few that I use:

- num: number of key-value pairs. This is the parameter to set when testing for large I/O (e.g. TBs of data)
- value_size: sets the value_size in bytes for each key-value pair. Can be used to check how well the store performs with different types of data. Can also be interesting to check the relation between e.g. blocksize and kv-pair size.
- key_size: sets the key_size in bytes for a key-value pair. This is interesting for similar reasons as the value_size, but also because keys are used in the metadata of e.g. SSTables. When the key size becomes very large, so does the metadata that comes along with it. This can be used to test if the key-value store support large keys and what quantity.
- compression_type: set it to `none`` when you do not want compression.
- write_buffer_size: size of the buffer that is used for the memtable. The bigger it is set, the more will be kept in RAM and the less flushes are needed (albeit larger flushes are done in the end).
- target_file_size_base: the approximate size to use for files. This can be used to for example get SSTables to be approximately the size of an SSD zone.
- histogram: very important. Shows a histogram at the end of each benchmark, that represents latencies. This is a good tool to get tail latencies of the key-value store.
- use_direct_reads: uses direct I/O for reads. A good parameter to set when you want to test storage.
- use_direct_io_for_flush_and_compaction: uses direct I/O for background operations. A good parameter to set when you want to test storage.
- max_background_jobs: can be used to tweak the maximum amount of background jobs that can be active. These threads can be used by e.g. compaction or flush threads, but might interfere with each other or the client threads.
- max_bytes_for_level_multiplier: The difference in max bytes for each level. Can be used to tweak the step between levels.
- threads: dependent on the benchmark. Generally means the client threads. For example for write benchmarks, it refers to concurrent client writes.
- use_existing_db: if set will use an already existing db. Some benchmarks require an existing db, others requires a new db. This parameter is good to set if multiple benchmarks will be run in series and they should continue on a dirty state.
- enable_pipelined_write: allows writing to memtable and WAL concurrently (like a pipeline).
- unordered_write + two_write_queues: relaxed consistency constraints for more performance. Is mutually exclusive with enable_pipelined_write. It is possible for concurrent reads to read different things and snapshots are not consistent (MVCC), but you will always read what you have just written.

* Benchmark RocksDB with ZenFS
ZenFS is an alternative file system that can only be used within RocksDB. It is essentially a domain-specific filesystem. Further on, it only works with ZNS SSDs. However, as of July 2022 this is the state of the art for RocksDB on such devices. It is therefore tempting to compare against ZenFS when benchmarking on ZNS SSDs.
** Installing ZenFS
Using ZenFS does require some extra installations. It must be used as a plugin for RocksDB, which also requires rebuilding RocksDB yourself. So it is not possible to use RocksDB from a package repository! At https://github.com/westerndigitalcorporation/zenfs, it is explained how you can setup ZenFS. Nevertheless, as is always the case with rebuilding from source, things can go wrong. As far as I am aware WD (the company behind ZenFS) has no direct influence over the updates of RocksDB and the team that works on RocksDB has no direct influence over ZenFS. They are essentially separate projects. Therefore there might be versioning inconsistencies. Whenever working with ZenFS always verify that the both versions are compatible and if you benchmark, make sure that you are using a combination that is "optimal". Preferably pin a commit, use this commit in your benchmarks and report the commit SHA in your experiment details.

Below I highlight some issues that I came across when using ZenFS.

The first issue is related to LibZBD. This is a dependency of ZenFS. This library should be compiled and installed manually. See https://github.com/westerndigitalcorporation/libzbd. Be sure that ZenFS is compatible with the version of LibZBD.

If you get errors to `undefined reference to google:FlagRegisterer::Flag...`, something probably went wrong with your gflags installation (you should already have a gflags installed as otherwise RocksDB would not compile). A good guide to solve this issue is https://github.com/gflags/gflags/issues/203 the answer by EricOops is a life saver. EricOops recommends building and installing `gflags` and `glog` manually, which are a ZenFS dependency. However, do not forget to first purge the current gflags installations just to be sure.

Another issue has to do with the built system of ZenFS. ZenFS makes use of the plugin functionality of RocksDB. RocksDB supports plugins in both UNIX Makefiles and CMake, but there functionality differs significantly. For example the CMake plugins do not support custom scripts and do not install plugin header files on an install. Further on, if installing using the Makefile, package-config files will be created as well, while they are not when using CMake. ZenFS relies on all of these functionalities. This means that if you altered RocksDB and only use the CMakeList you have a problem. If possible only use the Makefile.

Else, consider if it is possible to benchmark RocksDB with another installation. One that use the same version, but without your changes.

If you need the changes, there is a hack to circumvent the issue, but it is not advisable. Generally it might break, but it is a good way to debug your system. Do NOT do this for benchmarking. In that case just use the Makefile.
1. Go to the zenfs directory and call `./generate-version.sh`. This would ordinarily be called from the Makefile, which we will not use.
2. Create a CMakeLists.txt in this directory. We are going to create a CMake plugin for ZenFS. Use something like (depending on the ZenFS version). Try to mimick ZenFS' Makefile:
#+BEGIN_SRC cmake
cmake_minimum_required(VERSION 3.4)

set(zenfs_SOURCES "fs/fs_zenfs.cc" "fs/zbd_zenfs.cc" "fs/io_zenfs.cc" PARENT_SCOPE)
set(zenfs_HEADER_DIR "fs" PARENT_SCOPE)
set(zenfs_CMAKE_EXE_LINKER_FLAGS "-u zenfs_filesystem_reg" PARENT_SCOPE)
set(zenfs_LIBS "zbd" PARENT_SCOPE)
#+END_SRC
3. Go to the RocksDB directory. Alter the CMakeList to support plugin headers being copied as well: Before the `foreach(plugin \${PLUGINS})` add `set(PLUGIN_HEADER_DIR "")` and in the loop add `list(APPEND PLUGIN_HEADER_DIR "plugin/${plugin}/\${
   \${plugin}_HEADER_DIR}")`. Then in the `install` section (such as at `install(TARGETS ${ROCKSDB_STATIC_LIB}` add:
   #+BEGIN_SRC cmake
foreach(header_dir ${PLUGIN_HEADER_DIR})
    install(DIRECTORY ${header_dir} COMPONENT devel DESTINATION "${CMAKE_INSTALL_INCLUDEDIR}/rocksdb/plugin/zenfs")
endforeach()
   #+END_SRC
4. Create a copy of the project, but without your changes. Built and install it with the Makefile and ZenFS and copy the .pc file to a secure installation.
5. Deinstall that project.
6. Remove the CMakeCache of your own project and rebuild the project with the ZenFS plugin enabled. Now copy the location of the earlier generated .pc file to your PKGCONFIG path and create the ZenFS filesystem as usual. In this scenario, you will use all the configs that you use within CMake, BUT you will create the filesystem with a fallback method.

** ZenFS Benchmarking
Then after ZenFS is up and running. We should be able to do benchmarking. First pick an appropriate ZNS SSD and use its name. Pick the name in `/dev/<name>` as ZenFS automatically uses `/dev/`. Then do the following to setup ZenFS:
#+BEGIN_SRC bash
# In all lines <dev> is the devicename
echo deadline | sudo tee "/sys/block/$dev/queue/scheduler" # ZenFS requires deadline as a scheduler
rm -rf /tmp/zenfs-aux # ZenFS requires a temporary LOG file, but it is not allowed to already exist!
cd $ZENFS_DIR # ZENFS_DIR should be the utils directory of ZenFS
./zenfs mkfs -zbd=$dev -aux_path=/tmp/zenfs-aux
#+END_SRC
When this succeeds, you should see a message such as: "ZenFS file system created. Free space: 3968745 MB". Otherwise, assume that it has failed.

Now benchmarks can be run on ZenFS. For some good examples go to https://github.com/westerndigitalcorporation/zenfs/tree/master/tests. In particular look at `get_good_db_bench_params_for_zenfs.sh`. What is immediately noticeable is that using ZenFS requires different db_bench commands. You should modify the fs_uri to point to the ZenFS filesystem with the arg `-fs_uri=zenfs://dev:$dev` with `dev` the device name. Then it should already work, but it is not optimal. In addition, we should set the target filesize to equal approximately the size of a zone. This size should then be used in the arg: `--target_file_size_base`. The write buffersize, set with `--write_buffer_size`, should also approximate this size.
** Remove ZenFS filesystem from a device
This is very easy as the ZenFS filesystem is never mounted. It runs in user space. So you do not have to do anything. If however, you want the ZNS SSD to go to a clean state, reset all zones with:
#+BEGIN_SRC bash
nvme zns reset-zone /dev/$dev -a
#+END_SRC

* Benchmarking RocksDB with F2FS and ZNS SSDs
F2FS supports ZNS SSDs out of the box, provided a recent version of F2FS is used. However, it does require some additional setup and things to keep track of.The first idiosyncracy is that F2FS supports sequential zones for most of its data, except for at least a part of the metadata used. ZNS can support a few zones that can be written to randomly, but does not require to support them. Further on, such zones may not be enough to hold all metadata. Whenever the amount of randomly writable space is not enough, F2FS should warn you by default. For example, 100GB requires at least 4GB of random space and 7TB requires at least 16GB of random space. When the amount of space is not enough, we have to use an additional device as there is no other way. This does hinder benchmarks as F2FS "cheats" in this regard. To keep side-effects to a minimum, try to use a NVMe device with similar performance.

** Install F2FS
When using F2FS with F2fS-tools in 2022 and using the default kernel, ZNS is not supported by default. In that case, F2FS needs to be built manually. In this case, we have to be careful. Do NOT use the version on github as it does not seem to be maintained, instead clone from git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs-tools.git. Then it should be as simple as following the instructions from the repo. One thing to be aware of is if you have all dependencies properly installed and set. During the configuration phase (`/configure.sh``) you should see a list of capabilities with "yes" or "no" next to it. If "blkzoned.capacity" is no, you can create a ZNS file system (at least the command completes without errors or warnings), but you can not actually use it... In this case, be sure you have a modern kernel, the headers are installed and the kernel is built with all of the required configs. In my case I had to also update a few libraries, such as BPF.
** Create the file system
To create the filesystem on ZNS, Nick Tehrany has an excellent paper on how to use F2FS on ZNS at https://arxiv.org/abs/2206.01547. First ensure that the ZNS device is actually empty! As at the moment (June 2022) F2FS makes no attempt to reset the device with for example:
#+BEGIN_SRC bash
nvme zns reset-zone /dev/$dev -a replace dev with the ZNS device and do this for every namespace used by the filesystem.
#+END_SRC
After this it is sufficient to say:
#+BEGIN_SRC bash
mkfs.f2fs -f -m -c /dev/$devzns /dev/$devnvme # With devzns the seq-only ZNS namespace of a ZNS device and devnvme the randomly write-able namespace of a ZNS device (or an other ordinary device)
#+END_SRC bash
Then mount the filesystem at the preferred mount point, such as /mnt/f2fs with:
#+BEGIN_SRC bash
mount -t f2fs /dev/$devnvme /mnt/f2fs # with devnvme the device the randomly writeable area defined in the previous command.
#+END_SRC
** Benchmarking wtih F2FS
Be sure to use the pointed mounted with F2FS only, by specifying the db and wal directory with setting `--db` and `--wal_dir` to be directories within the mounted filesystem.
